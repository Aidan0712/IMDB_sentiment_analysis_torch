[0m

[ERRORS]
2025-10-26 19:59:39,006: INFO: running D:\2025Autumn\Scientific Research Training\Task3\imdb_sentiment_analysis_torch\imdb_roberta_trainer.py
C:\Users\Clumsy\AppData\Roaming\Python\Python312\site-packages\huggingface_hub\file_download.py:143: UserWarning:

`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Clumsy\.cache\huggingface\hub\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development


Map:   0%|          | 0/20000 [00:00<?, ? examples/s]
Map:   5%|â–Œ         | 1000/20000 [00:00<00:05, 3722.87 examples/s]
Map:  10%|â–ˆ         | 2000/20000 [00:00<00:03, 5424.32 examples/s]
Map:  15%|â–ˆâ–Œ        | 3000/20000 [00:00<00:02, 6317.88 examples/s]
Map:  20%|â–ˆâ–ˆ        | 4000/20000 [00:00<00:02, 6474.05 examples/s]
Map:  25%|â–ˆâ–ˆâ–Œ       | 5000/20000 [00:00<00:02, 6641.19 examples/s]
Map:  30%|â–ˆâ–ˆâ–ˆ       | 6000/20000 [00:00<00:02, 6754.75 examples/s]
Map:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7000/20000 [00:01<00:01, 6659.68 examples/s]
Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8000/20000 [00:01<00:01, 6615.91 examples/s]
Map:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9000/20000 [00:01<00:01, 6608.22 examples/s]
Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10000/20000 [00:01<00:01, 6851.59 examples/s]
Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11000/20000 [00:01<00:01, 6947.84 examples/s]
Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12000/20000 [00:01<00:01, 6880.68 examples/s]
Map:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13000/20000 [00:01<00:01, 6848.65 examples/s]
Map:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14000/20000 [00:02<00:00, 6799.59 examples/s]
Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15000/20000 [00:02<00:00, 6650.45 examples/s]
Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16000/20000 [00:02<00:00, 6425.52 examples/s]
Map:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17000/20000 [00:02<00:00, 6567.13 examples/s]
Map:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18000/20000 [00:02<00:00, 6800.95 examples/s]
Map:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19000/20000 [00:02<00:00, 6782.30 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:03<00:00, 6930.50 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:03<00:00, 6594.61 examples/s]

Map:   0%|          | 0/5000 [00:00<?, ? examples/s]
Map:  20%|â–ˆâ–ˆ        | 1000/5000 [00:00<00:00, 6775.64 examples/s]
Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2000/5000 [00:00<00:00, 6435.93 examples/s]
Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3000/5000 [00:00<00:00, 6650.54 examples/s]
Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4000/5000 [00:00<00:00, 6552.62 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 6333.66 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:00<00:00, 6397.34 examples/s]

Map:   0%|          | 0/25000 [00:00<?, ? examples/s]
Map:   4%|â–         | 1000/25000 [00:00<00:03, 6581.51 examples/s]
Map:   8%|â–Š         | 2000/25000 [00:00<00:03, 6337.13 examples/s]
Map:  12%|â–ˆâ–        | 3000/25000 [00:00<00:03, 6081.26 examples/s]
Map:  16%|â–ˆâ–Œ        | 4000/25000 [00:00<00:03, 6538.32 examples/s]
Map:  20%|â–ˆâ–ˆ        | 5000/25000 [00:00<00:03, 6610.12 examples/s]
Map:  24%|â–ˆâ–ˆâ–       | 6000/25000 [00:00<00:02, 6780.96 examples/s]
Map:  28%|â–ˆâ–ˆâ–Š       | 7000/25000 [00:01<00:02, 6941.91 examples/s]
Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 8000/25000 [00:01<00:02, 7098.45 examples/s]
Map:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9000/25000 [00:01<00:02, 6666.73 examples/s]
Map:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10000/25000 [00:01<00:02, 6682.54 examples/s]
Map:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11000/25000 [00:01<00:02, 5366.96 examples/s]
Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12000/25000 [00:01<00:02, 5736.47 examples/s]
Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13000/25000 [00:02<00:02, 5945.84 examples/s]
Map:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14000/25000 [00:02<00:01, 6305.95 examples/s]
Map:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15000/25000 [00:02<00:01, 6287.56 examples/s]
Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16000/25000 [00:02<00:01, 6601.86 examples/s]
Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17000/25000 [00:02<00:01, 6594.95 examples/s]
Map:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18000/25000 [00:02<00:01, 6596.89 examples/s]
Map:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19000/25000 [00:02<00:00, 6734.42 examples/s]
Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20000/25000 [00:03<00:00, 6736.42 examples/s]
Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21000/25000 [00:03<00:00, 6493.21 examples/s]
Map:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22000/25000 [00:03<00:00, 6682.56 examples/s]
Map:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23000/25000 [00:03<00:00, 6861.27 examples/s]
Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24000/25000 [00:03<00:00, 6850.47 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:03<00:00, 6710.60 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:03<00:00, 6501.00 examples/s]
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-10-26 19:59:58,221: WARNING: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "D:\2025Autumn\Scientific Research Training\Task3\imdb_sentiment_analysis_torch\imdb_roberta_trainer.py", line 48, in <module>
    metric = datasets.load_metric("accuracy")
             ^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'datasets' has no attribute 'load_metric'
